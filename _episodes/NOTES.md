## Hidden patterns

Ophthalmologists cannot classify the sex of a patient from a retinal scan. Radiologists cannot classify the race of a patient from a chest x-ray. In both cases, it appears that machine learning models can.

Examples, and discussion.

https://www.nature.com/articles/s41598-021-89743-x

https://arxiv.org/pdf/2107.10356.pdf

Models may not suffer from many of the weaknesses of humans, but they are no less susceptible to bias.

https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias





https://www.antidiskriminierungsstelle.de/EN/homepage/_documents/download_diskr_risiken_verwendung_von_algorithmen.pdf?__blob=publicationFile&v=1

<!--  TODO:

Task: read section X and answer questions.

# Debiasing data

https://www.abhishek-tiwari.com/bias-and-fairness-in-machine-learning/

-->


# Second case

<!--  TODO:

https://arxiv.org/pdf/1908.09635.pdf

-->


<!-- TODO: 

Another common cause of leakage is to mistakenly use information from the future when making a prediction. This is especially easy to do when working with retrospective data archived in relational databases, where the temporal aspects of data may be unclear. 

-->

exercises here: https://www.kaggle.com/alexisbcook/data-leakage

https://www.kaggle.com/scratchpad/notebookb0e15083f9/edit


<!-- TODO:



## Dataset shift

https://www.nejm.org/doi/full/10.1056/NEJMc2104626

Dataset shift describes the issue of data changing after a model is deployed. It is not enough to train a model and then let it be. After deployment, models need to be continuously monitored and adapted.

https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/60fb3ba110343004004f24ba/1627077538209/Performance_Gap___Prospective_Validation.pdf

https://proceedings.neurips.cc/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf

Examples, and discussion.

## Changing data

https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-020-00090-3

-->

<!--  
Task: read section X and answer questions.
-->


<!-- TODO:

## U.S. Black Lung Program

The U.S. Department of Labor administers the Federal Black Lung Program, an administrative system charged with managing claims by coal miners for workersâ€™ compensation for lung disease caused by coal mine dust.

https://www.itnonline.com/content/study-reveals-bias-among-doctors-who-classify-x-rays-coal-miners-black-lung-claims

https://www.atsjournals.org/doi/10.1513/AnnalsATS.202010-1350OC

# BIAS

Black patients scored less severe. 

https://www.nature.com/articles/d41586-019-03228-6

# Biased data

depixilizer:

https://www.bbc.com/news/technology-53165286#:~:text=A%20US%20university's%20claim%20it,a%20picture%20of%20their%20face%22.

-->

<!--

Ophthalmologists cannot classify the sex of a patient from a retinal scan. Radiologists cannot classify the race of a patient from a chest x-ray. In both cases, it appears that machine learning models can.

Examples, and discussion.

https://www.nature.com/articles/s41598-021-89743-x

https://arxiv.org/pdf/2107.10356.pdf


The research literature is now abundant with machine learning studies. It is fair to say that the quality of these studies is highly variable. 

https://www.bmj.com/content/369/bmj.m1328

-->